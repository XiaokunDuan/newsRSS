"""æ¯æ—¥æ–°é—»æŠ¥å‘Šç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
1. æŠ“å–æ‰€æœ‰æ–°é—»æº
2. ä½¿ç”¨ DeepSeek æ•´ç†åˆ†æ
3. å¤„ç†è¢«å®¡æŸ¥çš„å†…å®¹ï¼ˆå•ç‹¬ä¿å­˜ï¼‰
4. ç”Ÿæˆé‚®ä»¶æŠ¥å‘Š
5. ç”ŸæˆTelegramæŠ¥å‘Š
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict

from openai import OpenAI

from .config import Config
from .sources import NEWS_SOURCES
from .fetcher import NewsFetcher, NewsItem
from .bypass import PaywallBypass
from .email_sender import EmailSender
from .telegram_sender import TelegramSender
from .data_classes import ArticleResult, AnalysisConfig, DailyAnalysisSummary
from .article_analyzer import PerArticleAnalyzer
from .jsonl_writer import JSONLWriter
from .file_cleaner import FileCleaner

logger = logging.getLogger(__name__)


@dataclass
class PerArticleAnalysisResult:
    """é€ç¯‡åˆ†æç»“æœ"""
    success: bool
    results: list[ArticleResult] = None
    summary: Optional[str] = None
    censored_count: int = 0
    error_count: int = 0
    raw_news: Optional[list] = None
    stats: Optional[dict] = None


class DailyReportGenerator:
    """æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, config: Config, per_article_mode: bool = False):
        self.config = config
        self.per_article_mode = per_article_mode
        self.output_dir = config.output_dir

        if per_article_mode:
            # é€ç¯‡åˆ†ææ¨¡å¼é…ç½®
            self.analysis_config = AnalysisConfig(
                max_concurrent=config.per_article_max_concurrent,
                max_retries=config.per_article_max_retries,
                timeout_seconds=30,
                keep_days=config.per_article_keep_days,
                enable_auto_clean=config.per_article_enable_auto_clean,
                analysis_mode="per_article",
            )
            self.analyzer = PerArticleAnalyzer(config, self.analysis_config)
            self.writer = JSONLWriter(
                self.output_dir,
                subdir="articles",
                incremental_mode=True,
                deduplicate=True
            )
            self.cleaner = FileCleaner(
                self.output_dir,
                keep_days=config.per_article_keep_days,
                enable_auto_clean=config.per_article_enable_auto_clean,
            )
        else:
            # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼é…ç½®
            self.client = OpenAI(
                api_key=config.openai_api_key,
                base_url=config.openai_base_url,
            )
            self.model = config.openai_model
            self.censored_dir = self.output_dir / "censored"
            self.censored_dir.mkdir(parents=True, exist_ok=True)

    def _is_censored_response(self, response: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯å®¡æŸ¥æ‹’ç»çš„å›å¤"""
        if not self.per_article_mode:
            CENSORSHIP_INDICATORS = [
                "ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œæˆ‘è¿˜æ²¡å­¦ä¹ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥å‘æˆ‘é—®ä¸€äº›å…¶å®ƒçš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å¸®æ‚¨è§£å†³çš„ã€‚",
            ]
            response_lower = response.lower()
            for indicator in CENSORSHIP_INDICATORS:
                if indicator.lower() in response_lower:
                    return True
        return False

    def _call_llm(self, prompt: str, max_retries: int = 2) -> tuple[str, bool]:
        """è°ƒç”¨ LLMï¼Œè¿”å› (å“åº”, æ˜¯å¦è¢«å®¡æŸ¥)ï¼ˆä»…ç”¨äºæ‰¹é‡æ¨¡å¼ï¼‰"""
        if self.per_article_mode:
            # é€ç¯‡åˆ†ææ¨¡å¼ä¸‹ä¸ç›´æ¥è°ƒç”¨LLM
            return "é€ç¯‡åˆ†ææ¨¡å¼", False

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œå¸®åŠ©ç”¨æˆ·æ•´ç†å’Œåˆ†ææ¯æ—¥æ–°é—»ã€‚"
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=4000,
                )
                content = response.choices[0].message.content

                if self._is_censored_response(content):
                    return content, True

                return content, False

            except Exception as e:
                logger.warning(f"LLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return f"é”™è¯¯: {e}", False

        return "æœªçŸ¥é”™è¯¯", False

    async def fetch_all_news(self) -> list[NewsItem]:
        """æŠ“å–æ‰€æœ‰æ–°é—»"""
        logger.info("å¼€å§‹æŠ“å–æ–°é—»...")

        fetcher = NewsFetcher(self.config)

        bypass = PaywallBypass(
            proxy=self.config.http_proxy,
            timeout=35,
            bpc_extension_path=self.config.bpc_extension_path,
            use_browser_fallback=True,
        )

        try:
            # æŠ“å– RSS
            news_items = await fetcher.fetch_all_sources(NEWS_SOURCES)
            logger.info(f"æŠ“å–åˆ° {len(news_items)} æ¡æ–°é—»")

            # ä»˜è´¹å¢™å¤„ç†
            paywall_items = [n for n in news_items if n.has_paywall]
            if paywall_items:
                logger.info(f"å¤„ç† {len(paywall_items)} æ¡ä»˜è´¹å¢™æ–°é—»...")
                urls = [n.link for n in paywall_items[:30]]  # é™åˆ¶æ•°é‡
                results = await bypass.batch_get_articles(urls, max_concurrent=3)

                for item in paywall_items:
                    if item.link in results and results[item.link].success:
                        result = results[item.link]
                        if result.content:
                            item.full_content = result.content

            return news_items

        finally:
            await bypass.close()

    def _format_news_for_analysis(self, news_items: list[NewsItem]) -> str:
        """æ ¼å¼åŒ–æ–°é—»ç”¨äºåˆ†æ"""
        lines = []
        for i, item in enumerate(news_items[:50], 1):  # é™åˆ¶æ•°é‡
            content = item.full_content or item.summary or ""
            content = content[:500]  # æˆªæ–­
            lines.append(f"""
{i}. ã€{item.source_name}ã€‘{item.title}
   æ—¶é—´: {item.published or 'æœªçŸ¥'}
   æ‘˜è¦: {content}
""")
        return "\n".join(lines)

    def _generate_analysis_prompt(self, news_text: str) -> str:
        """ç”Ÿæˆåˆ†ææç¤ºè¯"""
        return f"""è¯·åˆ†æä»¥ä¸‹ä»Šæ—¥æ–°é—»ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥æ–°é—»ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. æŒ‰é‡è¦æ€§æ•´ç†å‡º 5-10 æ¡æœ€å€¼å¾—å…³æ³¨çš„æ–°é—»
2. å¯¹æ¯æ¡æ–°é—»ç»™å‡ºç®€çŸ­çš„ä¸­æ–‡æ€»ç»“ï¼ˆ1-2å¥è¯ï¼‰
3. åˆ†æè¿™äº›æ–°é—»èƒŒåçš„è¶‹åŠ¿å’Œæ½œåœ¨å½±å“
4. æœ€åç»™å‡ºä¸€äº›åŸºäºè¿™äº›æ–°é—»çš„æ€è€ƒå’Œå»ºè®®ï¼ˆå¯ä»¥æ¶‰åŠæŠ•èµ„ã€å·¥ä½œã€ç”Ÿæ´»ç­‰ä»»ä½•æ–¹é¢ï¼‰

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡
- é‡ç‚¹çªå‡ºï¼Œä¸è¦å†—ä½™
- å»ºè®®éƒ¨åˆ†è¦å…·ä½“å¯è¡Œ

ä»Šæ—¥æ–°é—»åˆ—è¡¨ï¼š
{news_text}

è¯·ç”Ÿæˆæ–°é—»ç®€æŠ¥ï¼š"""

    async def analyze_news(self, news_items: list[NewsItem]) -> PerArticleAnalysisResult:
        """åˆ†ææ–°é—»ï¼ˆæ”¯æŒé€ç¯‡å’Œæ‰¹é‡æ¨¡å¼ï¼‰"""
        if not news_items:
            return PerArticleAnalysisResult(success=False)

        if self.per_article_mode:
            # é€ç¯‡åˆ†ææ¨¡å¼
            logger.info(f"å¼€å§‹é€ç¯‡åˆ†æ {len(news_items)} æ¡æ–°é—»...")

            try:
                # é€ç¯‡åˆ†æ
                results = await self.analyzer.analyze_articles(news_items, detailed=True)

                # å†™å…¥ç»“æœ
                stats = self.writer.batch_write_results(results, atomic=True)

                # è·å–ç»Ÿè®¡ä¿¡æ¯
                analysis_stats = self.analyzer.get_summary_statistics(results)

                # åˆ›å»ºæ¯æ—¥æ‘˜è¦
                summary = DailyAnalysisSummary(
                    date=datetime.now().strftime("%Y-%m-%d"),
                    total_articles=len(news_items),
                    analyzed_articles=analysis_stats['analyzed_articles'],
                    censored_articles=analysis_stats['censored_articles'],
                    average_importance=analysis_stats['average_importance'],
                    top_categories=analysis_stats['category_distribution'],
                    jsonl_file=self.writer.articles_file,
                    censored_file=self.writer.censored_file,
                )

                # å†™å…¥æ‘˜è¦
                self.writer.write_summary(summary, incremental=True)

                # æ¸…ç†æ—§æ–‡ä»¶
                if self.config.per_article_enable_auto_clean:
                    cleanup_stats = self.cleaner.run_scheduled_cleanup(dry_run=False)
                    logger.info(f"è‡ªåŠ¨æ¸…ç†å®Œæˆ: {cleanup_stats['summary']}")

                return PerArticleAnalysisResult(
                    success=True,
                    results=results,
                    censored_count=analysis_stats['censored_articles'],
                    error_count=analysis_stats['error_count'],
                    raw_news=[asdict(n) for n in news_items[:50]],
                    stats=analysis_stats
                )

            except Exception as e:
                logger.error(f"é€ç¯‡åˆ†æå¤±è´¥: {e}")
                return PerArticleAnalysisResult(
                    success=False,
                    error_count=len(news_items),
                    raw_news=[asdict(n) for n in news_items[:50]],
                )
        else:
            # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼
            logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(news_items)} æ¡æ–°é—»...")

            # æ ¼å¼åŒ–æ–°é—»
            news_text = self._format_news_for_analysis(news_items)

            # ç”Ÿæˆæç¤ºè¯
            prompt = self._generate_analysis_prompt(news_text)

            # è°ƒç”¨ LLM
            response, censored = self._call_llm(prompt)

            if censored:
                logger.warning("åˆ†æè¢«å®¡æŸ¥ï¼Œä¿å­˜åŸå§‹æ–°é—»...")
                return PerArticleAnalysisResult(
                    success=False,
                    censored_count=len(news_items),
                    error="å†…å®¹è¢«å®¡æŸ¥",
                    raw_news=[asdict(n) for n in news_items[:50]]
                )

            return PerArticleAnalysisResult(
                success=True,
                summary=response,
                raw_news=[asdict(n) for n in news_items[:50]]
            )

    def save_censored_content(self, result: PerArticleAnalysisResult, date_str: str):
        """ä¿å­˜è¢«å®¡æŸ¥çš„å†…å®¹"""
        if self.per_article_mode:
            # é€ç¯‡åˆ†ææ¨¡å¼ä¸‹ï¼Œè¢«å®¡æŸ¥çš„å†…å®¹å·²ç»åœ¨JSONLæ–‡ä»¶ä¸­
            logger.info(f"é€ç¯‡åˆ†ææ¨¡å¼ä¸‹ï¼Œè¢«å®¡æŸ¥å†…å®¹å·²ä¿å­˜åœ¨JSONLæ–‡ä»¶ä¸­")
            return self.writer.censored_file
        else:
            # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼
            filepath = self.censored_dir / f"censored-{date_str}.json"

            data = {
                "date": date_str,
                "reason": result.error if hasattr(result, 'error') else "å†…å®¹è¢«å®¡æŸ¥",
                "news_count": len(result.raw_news) if result.raw_news else 0,
                "news": result.raw_news,
            }

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"è¢«å®¡æŸ¥å†…å®¹å·²ä¿å­˜: {filepath}")
            return filepath

    def save_report(self, summary: str, date_str: str) -> Path:
        """ä¿å­˜æŠ¥å‘Š"""
        filepath = self.output_dir / f"daily-report-{date_str}.md"

        content = f"""# æ¯æ—¥æ–°é—»ç®€æŠ¥

**æ—¥æœŸ**: {date_str}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{summary}

---

*æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚*
"""

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        return filepath

    def generate_email_html(self, summary: str, date_str: str) -> str:
        """ç”Ÿæˆé‚®ä»¶ HTML"""
        # ç®€å•çš„ Markdown è½¬ HTML
        html_content = summary.replace("\n\n", "</p><p>").replace("\n", "<br>")
        html_content = f"<p>{html_content}</p>"

        # å¤„ç†æ ‡é¢˜
        import re
        html_content = re.sub(r'###\s*(.+?)(<br>|</p>)', r'<h3>\1</h3>', html_content)
        html_content = re.sub(r'##\s*(.+?)(<br>|</p>)', r'<h2>\1</h2>', html_content)
        html_content = re.sub(r'#\s*(.+?)(<br>|</p>)', r'<h1>\1</h1>', html_content)

        # å¤„ç†åŠ ç²—
        html_content = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', html_content)

        return f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }}
        h1, h2, h3 {{
            color: #1a1a1a;
            margin-top: 24px;
        }}
        h1 {{
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }}
        p {{
            margin: 12px 0;
        }}
        .footer {{
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            color: #666;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <h1>ğŸ“° æ¯æ—¥æ–°é—»ç®€æŠ¥ - {date_str}</h1>
    {html_content}
    <div class="footer">
        <p>æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚</p>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
</body>
</html>
"""

    async def run(
        self,
        send_email: bool = True,
        send_telegram: bool = False,
        email_sender: Optional[str] = None,
        email_password: Optional[str] = None,
        email_recipient: Optional[str] = None,
        telegram_bot_token: Optional[str] = None,
        telegram_chat_id: Optional[str] = None,
    ) -> bool:
        """è¿è¡Œæ¯æ—¥æŠ¥å‘Šç”Ÿæˆ

        Args:
            send_email: æ˜¯å¦å‘é€é‚®ä»¶
            send_telegram: æ˜¯å¦å‘é€åˆ°Telegram
            email_sender: å‘ä»¶äººé‚®ç®±
            email_password: é‚®ç®±å¯†ç 
            email_recipient: æ”¶ä»¶äººé‚®ç®±
            telegram_bot_token: Telegram Bot Token
            telegram_chat_id: Telegram Chat ID

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        date_str = datetime.now().strftime("%Y-%m-%d")
        logger.info(f"å¼€å§‹ç”Ÿæˆ {date_str} æ¯æ—¥æŠ¥å‘Š (æ¨¡å¼: {'é€ç¯‡åˆ†æ' if self.per_article_mode else 'æ‰¹é‡åˆ†æ'})...")

        try:
            # 1. æŠ“å–æ–°é—»
            news_items = await self.fetch_all_news()

            if not news_items:
                logger.error("æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°é—»")
                return False

            # 2. åˆ†ææ–°é—»
            result = await self.analyze_news(news_items)

            # 3. å¤„ç†ç»“æœ
            if not result.success:
                if self.per_article_mode:
                    # é€ç¯‡æ¨¡å¼ä¸‹ï¼Œå³ä½¿éƒ¨åˆ†å¤±è´¥ï¼Œå¦‚æœåˆ†æè¿‡ä¸€äº›æ–‡ç« ä¹Ÿç®—æˆåŠŸ
                    if result.results and len(result.results) > 0:
                        logger.warning(f"é€ç¯‡åˆ†æéƒ¨åˆ†å¤±è´¥: æˆåŠŸ {len(result.results)} ç¯‡ï¼Œå¤±è´¥ {result.error_count} ç¯‡")
                        # ç»§ç»­å¤„ç†æˆåŠŸçš„ç»“æœ
                    else:
                        logger.error(f"åˆ†æå®Œå…¨å¤±è´¥: {result.error_count} ç¯‡å…¨éƒ¨å¤±è´¥")
                        return False
                else:
                    # æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œå¤±è´¥åˆ™æ•´ä½“å¤±è´¥
                    logger.error(f"åˆ†æå¤±è´¥: {result.error}")
                    return False

            # 4. ä¿å­˜æŠ¥å‘Š
            if self.per_article_mode:
                # é€ç¯‡æ¨¡å¼ä¸‹ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
                if result.results:
                    # ä»JSONLæ–‡ä»¶è¯»å–æˆåŠŸåˆ†æçš„æ–‡ç« 
                    articles = self.writer.read_articles()
                    if articles:
                        summary_text = self._generate_summary_from_jsonl(articles)
                        self.save_report(summary_text, date_str)
                        logger.info(f"ç”Ÿæˆæ‘˜è¦æŠ¥å‘Šï¼ŒåŸºäº {len(articles)} ç¯‡æˆåŠŸåˆ†æçš„æ–‡ç« ")
                    else:
                        logger.warning("æ²¡æœ‰æˆåŠŸåˆ†æçš„æ–‡ç« ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")

                    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                    progress_info = self.writer.get_progress_info()
                    logger.info(f"å¤„ç†è¿›åº¦: {progress_info['processed_count']} ç¯‡æ–‡ç« å·²å¤„ç†")
            else:
                # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼
                self.save_report(result.summary, date_str)

            # 5. å‘é€é‚®ä»¶
            if send_email and email_sender and email_password and email_recipient:
                sender = EmailSender(email_sender, email_password)
                if self.per_article_mode:
                    # é€ç¯‡åˆ†æé‚®ä»¶å†…å®¹
                    if result.stats:
                        email_content = self._generate_per_article_email_content(result.stats, date_str)
                        email_subject = f"ğŸ“° é€ç¯‡æ–°é—»åˆ†æ - {date_str}"
                    else:
                        email_content = f"ä»Šæ—¥é€ç¯‡æ–°é—»åˆ†æå®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ã€‚"
                        email_subject = f"[æ–°é—»åˆ†æ] {date_str} - å®Œæˆ"
                else:
                    # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼é‚®ä»¶å†…å®¹
                    email_content = self.generate_email_html(result.summary, date_str)
                    email_subject = f"ğŸ“° æ¯æ—¥æ–°é—»ç®€æŠ¥ - {date_str}"

                success = sender.send(
                    email_recipient,
                    email_subject,
                    email_content,
                    is_html=True if not self.per_article_mode else False,
                )
                if not success:
                    logger.error("é‚®ä»¶å‘é€å¤±è´¥")

            # 6. å‘é€åˆ°Telegram
            if send_telegram and telegram_bot_token and telegram_chat_id:
                try:
                    # æ‰¾åˆ°ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
                    report_files = list(self.output_dir.glob(f"daily-report-{date_str}.*"))
                    if not report_files:
                        # å¦‚æœæ˜¯é€ç¯‡åˆ†ææ¨¡å¼ï¼Œå‘é€ç»Ÿè®¡ä¿¡æ¯
                        if self.per_article_mode and result.stats:
                            stats_text = self._format_stats_for_telegram(result.stats, date_str)
                            telegram_sender = TelegramSender(telegram_bot_token, telegram_chat_id)
                            telegram_success = await telegram_sender.send_text(stats_text, lines_per_chunk=20)
                            if telegram_success:
                                logger.info(f"Telegramç»Ÿè®¡ä¿¡æ¯å‘é€æˆåŠŸ")
                            else:
                                logger.error("Telegramç»Ÿè®¡ä¿¡æ¯å‘é€å¤±è´¥")
                        else:
                            logger.warning("æœªæ‰¾åˆ°æ¯æ—¥æŠ¥å‘Šæ–‡ä»¶ï¼Œè·³è¿‡Telegramå‘é€")
                    else:
                        # å‘é€æŠ¥å‘Šæ–‡ä»¶å†…å®¹
                        for report_file in report_files:
                            telegram_sender = TelegramSender(telegram_bot_token, telegram_chat_id)
                            telegram_success = await telegram_sender.send_file_chunks(
                                report_file,
                                lines_per_chunk=15
                            )
                            if telegram_success:
                                logger.info(f"TelegramæŠ¥å‘Šå‘é€æˆåŠŸ: {report_file.name}")
                            else:
                                logger.error(f"TelegramæŠ¥å‘Šå‘é€å¤±è´¥: {report_file.name}")

                except Exception as e:
                    logger.error(f"Telegramå‘é€å¤±è´¥: {e}")

            logger.info("æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return False

    def _format_stats_for_telegram(self, stats: dict, date_str: str) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯ç”¨äºTelegramå‘é€"""
        return f"ğŸ“Š æ¯æ—¥æ–°é—»åˆ†æç»Ÿè®¡ - {date_str}\n\n" + \
               f"ğŸ“° æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}\n" + \
               f"âœ… æˆåŠŸåˆ†æ: {stats.get('analyzed_articles', 0)}\n" + \
               f"âš ï¸ è¢«å®¡æŸ¥: {stats.get('censored_articles', 0)}\n" + \
               f"â­ å¹³å‡é‡è¦æ€§: {stats.get('average_importance', 0):.1f}/10\n\n" + \
               f"ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:\n" + \
               self._format_category_distribution(stats.get('category_distribution', {}))

    def _format_category_distribution(self, distribution: dict) -> str:
        """æ ¼å¼åŒ–ç±»åˆ«åˆ†å¸ƒ"""
        if not distribution:
            return "   æ— ç±»åˆ«æ•°æ®"

        lines = []
        for category, count in sorted(distribution.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"    â€¢ {category}: {count} ç¯‡")
        return '\n'.join(lines)

    def _generate_summary_from_jsonl(self, articles: list[dict]) -> str:
        """ä»JSONLæ•°æ®ç”Ÿæˆæ‘˜è¦"""
        # æå–æœ€é‡è¦çš„æ–‡ç« 
        important_articles = sorted(
            [a for a in articles if a.get('importance', 0) >= 7],
            key=lambda x: x.get('importance', 0),
            reverse=True
        )[:10]

        lines = ["# ä»Šæ—¥é‡è¦æ–°é—»æ‘˜è¦\n"]
        for i, article in enumerate(important_articles, 1):
            lines.append(f"\n## {i}. {article.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
            lines.append(f"**æ¥æº**: {article.get('source_name', 'æœªçŸ¥')}")
            lines.append(f"**é‡è¦æ€§**: {article.get('importance', 5)}/10")
            if article.get('summary'):
                lines.append(f"**æ‘˜è¦**: {article.get('summary')}")
            if article.get('key_points'):
                lines.append(f"**å…³é”®ç‚¹**:")
                for point in article.get('key_points', [])[:3]:
                    lines.append(f"  â€¢ {point}")

        return '\n'.join(lines)

    def _generate_per_article_email_content(self, stats: dict, date_str: str) -> str:
        """ç”Ÿæˆé€ç¯‡åˆ†æé‚®ä»¶å†…å®¹"""
        return f"""ä»Šæ—¥é€ç¯‡æ–°é—»åˆ†ææŠ¥å‘Š - {date_str}

ç»Ÿè®¡åˆ†æ:
â€¢ æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}
â€¢ æˆåŠŸåˆ†æ: {stats.get('analyzed_articles', 0)}
â€¢ è¢«å®¡æŸ¥: {stats.get('censored_articles', 0)}
â€¢ å¹³å‡é‡è¦æ€§: {stats.get('average_importance', 0):.1f}

ç±»åˆ«åˆ†å¸ƒ:
{self._format_category_distribution(stats.get('category_distribution', {}))}

è¯¦ç»†ç»“æœå·²ä¿å­˜åœ¨ JSONL æ–‡ä»¶ä¸­ï¼Œå¯è¿›è¡Œåç»­åˆ†æã€‚
"""


async def run_daily_report(
    config: Config,
    per_article_mode: bool = False,
    send_telegram: bool = False,
) -> bool:
    """è¿è¡Œæ¯æ—¥æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°

    Args:
        config: é…ç½®
        per_article_mode: æ˜¯å¦å¯ç”¨é€ç¯‡åˆ†ææ¨¡å¼
        send_telegram: æ˜¯å¦å‘é€åˆ°Telegram

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    generator = DailyReportGenerator(config, per_article_mode=per_article_mode)
    return await generator.run(
        send_email=True,
        send_telegram=send_telegram,
        email_sender=config.email_sender,
        email_password=config.email_password,
        email_recipient=config.email_recipient,
        telegram_bot_token=config.telegram_bot_token,
        telegram_chat_id=config.telegram_chat_id,
    )
